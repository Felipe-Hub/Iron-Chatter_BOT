{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing casual conversation dataset\n",
    "\n",
    "dir_path = 'Conversation'\n",
    "files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "for filepath in files_list:\n",
    "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len( con ) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[ 1 : ]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append( ans )\n",
    "        elif len( con )> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 1894\n"
     ]
    }
   ],
   "source": [
    "answers_with_tags = list()\n",
    "for i in range( len( answers ) ):\n",
    "    if type( answers[i] ) == str:\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop( i )\n",
    "\n",
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "vocab_size = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( vocab_size ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564, 22) 22\n",
      "(564, 74) 74\n",
      "(564, 74, 1894)\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "    vocab.append( word )\n",
    "\n",
    "def tokenize( sentences ):\n",
    "    tokens_list = []\n",
    "    vocabulary = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n",
    "        tokens = sentence.split()\n",
    "        vocabulary += tokens\n",
    "        tokens_list.append( tokens )\n",
    "    return tokens_list , vocabulary\n",
    "\n",
    "p = tokenize( questions + answers )\n",
    "model = Word2Vec( p[ 0 ] ) \n",
    "\n",
    "embedding_matrix = np.zeros( ( vocab_size , 100 ) )\n",
    "for i in range( len( tokenizer.word_index ) ):\n",
    "    if i in model.wv.vocab.keys():\n",
    "        embedding_matrix[ i ] = model[ vocab[i] ]\n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , vocab_size )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    484864      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    484864      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1894)   486758      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,507,110\n",
      "Trainable params: 2,507,110\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( vocab_size, 256 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 256 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( vocab_size, 256 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 256 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( vocab_size , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.002), metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 451 samples, validate on 113 samples\n",
      "Epoch 1/20\n",
      "451/451 [==============================] - 289s 640ms/sample - loss: 1.5450 - acc: 0.0440 - val_loss: 1.0998 - val_acc: 0.1597\n",
      "Epoch 2/20\n",
      "451/451 [==============================] - 103s 227ms/sample - loss: 1.5415 - acc: 0.1137 - val_loss: 1.0949 - val_acc: 0.1679\n",
      "Epoch 3/20\n",
      "451/451 [==============================] - 52s 115ms/sample - loss: 1.5285 - acc: 0.1125 - val_loss: 1.0679 - val_acc: 0.1818\n",
      "Epoch 4/20\n",
      "451/451 [==============================] - 38s 85ms/sample - loss: 1.4539 - acc: 0.1228 - val_loss: 0.9888 - val_acc: 0.1343\n",
      "Epoch 5/20\n",
      "451/451 [==============================] - 31s 69ms/sample - loss: 1.3201 - acc: 0.0821 - val_loss: 0.9046 - val_acc: 0.1032\n",
      "Epoch 6/20\n",
      "451/451 [==============================] - 27s 61ms/sample - loss: 1.2283 - acc: 0.0710 - val_loss: 0.8969 - val_acc: 0.0983\n",
      "Epoch 7/20\n",
      "451/451 [==============================] - 47s 103ms/sample - loss: 1.2102 - acc: 0.0682 - val_loss: 0.9191 - val_acc: 0.0925\n",
      "Epoch 8/20\n",
      "451/451 [==============================] - 40s 88ms/sample - loss: 1.2015 - acc: 0.0688 - val_loss: 0.9361 - val_acc: 0.1196\n",
      "Epoch 9/20\n",
      "451/451 [==============================] - 41s 92ms/sample - loss: 1.1864 - acc: 0.0721 - val_loss: 0.9402 - val_acc: 0.0328\n",
      "Epoch 10/20\n",
      "451/451 [==============================] - 33s 72ms/sample - loss: 1.1734 - acc: 0.0803 - val_loss: 0.9422 - val_acc: 0.1859\n",
      "Epoch 11/20\n",
      "451/451 [==============================] - 34s 75ms/sample - loss: 1.1698 - acc: 0.1317 - val_loss: 0.9421 - val_acc: 0.1859\n",
      "Epoch 12/20\n",
      "451/451 [==============================] - 35s 78ms/sample - loss: 1.1659 - acc: 0.1320 - val_loss: 0.9407 - val_acc: 0.1859\n",
      "Epoch 13/20\n",
      "451/451 [==============================] - 135s 299ms/sample - loss: 1.1596 - acc: 0.1320 - val_loss: 0.9420 - val_acc: 0.1859\n",
      "Epoch 14/20\n",
      "451/451 [==============================] - 139s 309ms/sample - loss: 1.1554 - acc: 0.1293 - val_loss: 0.9457 - val_acc: 0.1564\n",
      "Epoch 15/20\n",
      "451/451 [==============================] - 107s 237ms/sample - loss: 1.1527 - acc: 0.0853 - val_loss: 0.9468 - val_acc: 0.1253\n",
      "Epoch 16/20\n",
      "451/451 [==============================] - 80s 178ms/sample - loss: 1.1474 - acc: 0.0805 - val_loss: 0.9454 - val_acc: 0.1269\n",
      "Epoch 17/20\n",
      "451/451 [==============================] - 123s 272ms/sample - loss: 1.1429 - acc: 0.0853 - val_loss: 0.9429 - val_acc: 0.1482\n",
      "Epoch 18/20\n",
      "451/451 [==============================] - 49s 109ms/sample - loss: 1.1392 - acc: 0.0897 - val_loss: 0.9387 - val_acc: 0.1646\n",
      "Epoch 19/20\n",
      "451/451 [==============================] - 114s 252ms/sample - loss: 1.1340 - acc: 0.0957 - val_loss: 0.9334 - val_acc: 0.1777\n",
      "Epoch 20/20\n",
      "451/451 [==============================] - 35s 77ms/sample - loss: 1.1276 - acc: 0.1011 - val_loss: 0.9283 - val_acc: 0.1867\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=256,\n",
    "          epochs=20, shuffle=True, validation_split=0.2) \n",
    "\n",
    "model.save( 'model_conversation.h5' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 256 ,)) # max sequence length\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        word = word.strip(\"',.:;?$\")\n",
    "        if word in tokenizer.word_index:\n",
    "            tokens_list.append( tokenizer.word_index[ word ] )\n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : hi\n",
      "i !\n",
      "end !\n",
      " i end\n",
      "Enter question : can you answer a question\n",
      "end !\n",
      " end\n",
      "Enter question : bye\n",
      "Hope to see you soon!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    inp = input( 'Enter question : ' )\n",
    "    \n",
    "    if inp.lower() == 'bye' or inp.lower() == 'end':\n",
    "        print('Hope to see you soon!')\n",
    "        break\n",
    "        \n",
    "    states_values = enc_model.predict( str_to_tokens( inp ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "   \n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            #print(word,index)\n",
    "            if sampled_word_index == index :\n",
    "                # print(word, '!')\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "    \n",
    "    print( decoded_translation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = str_to_tokens('are you alive yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11   3 832  88   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_values = enc_model.predict( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.9250318 ,  0.6752312 ,  0.95101345, -0.9964426 , -0.9895105 ,\n",
       "          0.9663247 , -0.994699  ,  0.9922562 , -0.9918174 ,  0.95332503,\n",
       "          0.99505883, -0.99545527,  0.9957602 ,  0.9963229 ,  0.91156596,\n",
       "         -0.9748242 ,  0.3386685 , -0.99504113, -0.9936898 , -0.97976625,\n",
       "          0.9547825 , -0.9956732 , -0.992852  , -0.28045473,  0.9968604 ,\n",
       "         -0.9937114 , -0.01042464, -0.90232706, -0.9943071 ,  0.9945505 ,\n",
       "         -0.99429774, -0.9768969 ,  0.6647353 ,  0.06206416, -0.97602874,\n",
       "         -0.99444723,  0.9945752 , -0.9961743 , -0.80914587,  0.9963387 ,\n",
       "          0.99650925,  0.9887442 , -0.98992264, -0.8152563 ,  0.21929361,\n",
       "          0.85932785, -0.96494603,  0.00226489, -0.98585206,  0.02420429,\n",
       "         -0.9926739 , -0.67397463, -0.95304936, -0.9910338 ,  0.9617914 ,\n",
       "          0.8042295 , -0.72745275,  0.96770704, -0.01225162, -0.9968767 ,\n",
       "         -0.66481733, -0.95352626,  0.09174662, -0.96543753,  0.9945669 ,\n",
       "          0.00345756, -0.94600946,  0.989601  , -0.9503441 , -0.9896021 ,\n",
       "         -0.1201397 , -0.9935455 , -0.98456645, -0.8589895 ,  0.46336126,\n",
       "          0.99521726, -0.68223375,  0.66485745, -0.00420148,  0.18412232,\n",
       "         -0.9439274 , -0.99219424, -0.996759  , -0.9961808 , -0.9943034 ,\n",
       "          0.30331287, -0.04955332,  0.98414   , -0.9939227 ,  0.993615  ,\n",
       "         -0.9839399 , -0.05166855,  0.97837317,  0.9612647 ,  0.9975209 ,\n",
       "         -0.9960571 ,  0.98483473, -0.99558616,  0.9969546 ,  0.53572905,\n",
       "          0.9956413 ,  0.9757756 ,  0.99254924,  0.00952552, -0.948656  ,\n",
       "          0.00153142, -0.00238723, -0.3994223 , -0.99522734,  0.99631715,\n",
       "          0.99284863,  0.9927119 ,  0.88086337, -0.9144503 ,  0.01136777,\n",
       "         -0.9882832 ,  0.9976949 ,  0.01990022,  0.9957026 ,  0.9955407 ,\n",
       "         -0.9888812 , -0.9310274 , -0.9940231 , -0.9944529 ,  0.8993941 ,\n",
       "          0.31815156,  0.99467665,  0.9682534 , -0.9506545 ,  0.99468654,\n",
       "          0.8625681 ,  0.6915404 , -0.9934115 , -0.99655014,  0.9781544 ,\n",
       "          0.98632973, -0.06843846,  0.03597598,  0.00256143,  0.24107784,\n",
       "          0.9791813 ,  0.9728105 ,  0.9867772 ,  0.9963176 , -0.81442916,\n",
       "         -0.9688837 , -0.65691817,  0.74216217, -0.96031916,  0.9176448 ,\n",
       "          0.99656755, -0.9082893 , -0.94829184,  0.9617531 ,  0.9966169 ,\n",
       "          0.9961865 ,  0.99547005,  0.9882028 , -0.9867018 ,  0.9933903 ,\n",
       "          0.00939461,  0.22107846,  0.9912455 , -0.9840185 ,  0.9943683 ,\n",
       "         -0.99663943,  0.9942245 ,  0.99633855,  0.9963688 ,  0.18771933,\n",
       "         -0.9968398 , -0.97770584, -0.99605197, -0.9907278 , -0.82532954,\n",
       "         -0.9944039 ,  0.9551091 ,  0.86042535,  0.99336046,  0.95943445,\n",
       "         -0.9962044 ,  0.993641  ,  0.00684269,  0.9841484 ,  0.99517316,\n",
       "          0.07890944, -0.9947857 ,  0.5154668 , -0.99507505, -0.9865866 ,\n",
       "          0.9876664 ,  0.9050781 ,  0.9845887 ,  0.3665295 , -0.9863716 ,\n",
       "         -0.17432252,  0.8569405 , -0.9958029 , -0.98423266,  0.9577854 ]],\n",
       "       dtype=float32),\n",
       " array([[-1.6235392 ,  1.6217635 ,  1.8578839 , -3.18337   , -2.8688235 ,\n",
       "          2.0438943 , -2.9818804 ,  2.8563204 , -2.7876472 ,  2.5634084 ,\n",
       "          3.0206244 , -3.1031952 ,  3.1463535 ,  3.209856  ,  1.5893677 ,\n",
       "         -2.2010736 ,  0.35265166, -3.0988503 , -3.0094066 , -2.3650844 ,\n",
       "          1.9526095 , -3.1181495 , -3.065487  , -0.6097577 ,  3.2366133 ,\n",
       "         -2.991635  , -3.0802336 , -1.5223316 , -2.9923344 ,  3.0994034 ,\n",
       "         -3.1254776 , -2.4504328 ,  1.1868272 ,  2.886188  , -3.3000073 ,\n",
       "         -2.961605  ,  3.0189748 , -3.1605446 , -1.1364106 ,  3.1961317 ,\n",
       "          3.2280931 ,  2.5886056 , -2.854433  , -1.1707699 ,  2.5739088 ,\n",
       "          3.2242384 , -2.014319  ,  3.0365038 , -2.5439098 ,  3.1071563 ,\n",
       "         -3.0804791 , -1.2864332 , -2.2483015 , -2.8303833 ,  1.9885201 ,\n",
       "          1.1962818 , -0.9263196 ,  2.8920097 , -2.729261  , -3.2501404 ,\n",
       "         -0.90615803, -1.9461056 ,  0.27819085, -2.7561448 ,  3.0059247 ,\n",
       "          0.00350161, -3.006776  ,  2.6921866 , -1.841814  , -3.0297232 ,\n",
       "         -0.12288294, -2.9255166 , -3.2050405 , -3.010347  ,  1.0009725 ,\n",
       "          3.1168604 , -0.8434528 ,  1.1847154 , -2.7906284 ,  1.2963463 ,\n",
       "         -2.532986  , -2.8878498 , -3.2732387 , -3.1618624 , -2.9615188 ,\n",
       "          0.31649947, -3.1877546 ,  2.6131537 , -2.9933674 ,  2.9558935 ,\n",
       "         -3.039341  , -0.6563703 ,  3.172869  ,  3.1344714 ,  3.464737  ,\n",
       "         -3.162745  ,  2.5801663 , -3.2009504 ,  3.2570739 ,  0.6013727 ,\n",
       "          3.1912177 ,  2.201669  ,  2.8680277 ,  3.213449  , -2.8177357 ,\n",
       "          2.7405527 , -3.1109185 , -0.42379934, -3.0492628 ,  3.1850448 ,\n",
       "          2.9275875 ,  3.0316734 ,  1.4887038 , -1.5544202 ,  2.37781   ,\n",
       "         -3.069571  ,  3.4015284 ,  2.9263568 ,  3.1025927 ,  3.1027179 ,\n",
       "         -2.932666  , -2.9533477 , -2.932473  , -3.034417  ,  3.0396512 ,\n",
       "          3.0649793 ,  2.9806669 ,  2.0964885 , -2.0840774 ,  3.1772757 ,\n",
       "          2.5276253 ,  2.2778509 , -3.04318   , -3.1879764 ,  2.2568064 ,\n",
       "          2.5010378 , -0.06912705,  0.6950432 ,  0.84721565,  0.25585043,\n",
       "          2.2969723 ,  2.1448991 ,  2.5517223 ,  3.1967719 , -1.1584375 ,\n",
       "         -2.0747232 , -0.79033566,  1.0076655 , -2.9478564 ,  1.5909523 ,\n",
       "          3.2250512 , -2.9129472 , -1.8215076 ,  1.9990728 ,  3.2088833 ,\n",
       "          3.185751  ,  3.1107965 ,  2.8530164 , -2.5368037 ,  3.0034442 ,\n",
       "          2.5349975 ,  2.9948366 ,  2.9467516 , -2.9210997 ,  2.9844084 ,\n",
       "         -3.2072606 ,  3.0908341 ,  3.1825142 ,  3.2271204 ,  2.8322902 ,\n",
       "         -3.236123  , -2.761795  , -3.1560056 , -2.7296505 , -1.1750088 ,\n",
       "         -3.0164156 ,  1.9159918 ,  1.9089444 ,  2.9966106 ,  1.9733075 ,\n",
       "         -3.2037513 ,  2.917828  ,  2.1616905 ,  2.9831643 ,  3.0673828 ,\n",
       "          0.7871888 , -3.0660248 ,  2.9897943 , -3.045818  , -2.5848825 ,\n",
       "          2.6408162 ,  1.536062  ,  2.7786188 ,  0.3856025 , -2.6265938 ,\n",
       "         -2.5027785 ,  1.2847645 , -3.216151  , -3.0103655 ,  2.0755706 ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.6642572e-05, 3.1620646e-05, 1.9246917e-03, ...,\n",
       "         3.3558518e-04, 3.7239853e-04, 4.1406759e-04]]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
