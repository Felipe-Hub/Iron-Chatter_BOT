{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potions: re, numpy as np, pandas as pd, pickle, json, nltk, keras, collections\n",
      "spells: clean_data, predict_tags, tagged_n_grams\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Bidirectional, Embedding\n",
    "\n",
    "import random\n",
    "\n",
    "from NLP_little_helpers import *\n",
    "little_helpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "\n",
    "model = load_model('model_full_concat.h5')\n",
    "\n",
    "with open('hmm_model.pkl', 'rb') as f:\n",
    "    hmm_model = pickle.load(f)\n",
    "\n",
    "with open('text_tokenizer.pkl', 'rb') as f:\n",
    "    tk_text = pickle.load(f)\n",
    "    \n",
    "with open('tags_tokenizer.pkl', 'rb') as f:\n",
    "    tk_tags = pickle.load(f)\n",
    "    \n",
    "with open('max_length.pkl', 'rb') as f:\n",
    "    max_length = pickle.load(f)\n",
    "    \n",
    "with open('w2v.pkl', 'rb') as f:\n",
    "    w2v = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i cant figure this out can you help me pls\n"
     ]
    }
   ],
   "source": [
    "sent = clean_data([input()])\n",
    "tags = [predict_tags(sent, hmm_model)['tagged'][0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_sentences = tk_text.texts_to_sequences(sent)\n",
    "dec_tagged = tk_tags.texts_to_sequences(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_sentences=[]\n",
    "for i, word in enumerate(sent[0].split()):\n",
    "    if word not in tk_text.word_index:\n",
    "        unk_sentences.append('<unk>')\n",
    "    else:\n",
    "        unk_sentences.append(tk_text.word_index[word])\n",
    "\n",
    "if '<unk>' not in unk_sentences:\n",
    "    print(\"You do not need to go further, all words are in vocabulary!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 4, '<unk>', 752, 25, 46, 55, 3, 162, 19, '<unk>', 2],\n",
       " ['<bos> i cant figure this out can you help me pls <eos>'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = (unk_sentences, dec_tagged[0])\n",
    "unk_sentences, sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged\n",
    "tk_text.word_index['me']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4,  539,    3, ..., 1033,  187, 2391], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reversed dictionary to find word by their sequence code\n",
    "seq2word_map = dict(map(reversed, tk_text.word_index.items()))\n",
    "\n",
    "# keeping the same n-grams as trained data\n",
    "n = max_length\n",
    "\n",
    "# enco\n",
    "X = unk_n_grams(tagged, n)\n",
    "\n",
    "for i in X:\n",
    "    \n",
    "\n",
    "prob_vec = model.predict(X)\n",
    "\n",
    "y_hat = np.argsort(-prob_vec)[0]\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>\n",
      "i\n",
      "figure\n",
      "this\n",
      "out\n",
      "can\n",
      "you\n",
      "help\n",
      "me\n",
      "<eos>\n",
      "you\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "for word in unk_sentences:\n",
    "    if word != '<unk>':\n",
    "        print(seq2word_map[word])\n",
    "    else:\n",
    "        print(seq2word_map[y_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, max_length, seed_text):\n",
    "    if seed_text == \"\":\n",
    "    []    return \"\"\n",
    "    else:\n",
    "        in_text = seed_text\n",
    "        n_words = 1\n",
    "        n_preds = 5 #number of words to predict for the seed text\n",
    "        pred_words = \"\"\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            # pre-pad sequences to a fixed length\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "            # predict probabilities for each word\n",
    "            proba = model.predict(encoded, verbose=0).flatten()\n",
    "            #take the n_preds highest probability classes \n",
    "            yhat = numpy.argsort(-proba)[:n_preds] \n",
    "            # map predicted words index to word\n",
    "            out_word = ''\n",
    "\n",
    "            for _ in range(n_preds):\n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index == yhat[_] and word not in stopwords:\n",
    "                        out_word = word\n",
    "                        pred_words += ' ' + out_word\n",
    "                        #print(out_word)\n",
    "                        break\n",
    "\n",
    "\n",
    "        return pred_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find and set embeddings for OOV words\n",
    "def set_embedding_for_oov(doc):\n",
    "    #checking for oov words and adding embedding\n",
    "    for token in doc:\n",
    "        if token.is_oov == True:\n",
    "            before_text = doc[:token.i].text\n",
    "            after_text = str(array(doc)[:token.i:-1]).replace('[','').replace(']','')\n",
    "\n",
    "            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
    "            pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
    "            \n",
    "            embedding = numpy.zeros((300,))\n",
    "\n",
    "            i=len(before_text)\n",
    "            print('Words predicted from forward sequence model:')\n",
    "            for word in pred_before:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            i=len(after_text)\n",
    "            print('Words predicted from reverse sequence model:')\n",
    "            for word in pred_after:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            nlp.vocab.set_vector(token.text, embedding)\n",
    "            print(token.text,nlp.vocab.get_vector(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('i livwgffe in london ')\n",
    "set_embedding_for_oov(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(nlp('livwgffe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word):\n",
    "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [w.orth_ for w in by_similarity[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = nlp('i live in lndn ')\n",
    "set_embedding_for_oov(test1)\n",
    "nlp.vocab.get_vector('lndn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('lndn').similarity(nlp('London'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(nlp('lndn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = nlp('i play fidditch at school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_embedding_for_oov(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(nlp('fidditch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('fidditch').similarity(nlp('sport'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 55,  46,  25, 752]),\n",
       "  array([14, 27, 22,  1]),\n",
       "  array([0, 0, 1, 4]),\n",
       "  array([0, 0, 2, 1])),\n",
       " (array([0, 0, 0, 2]),\n",
       "  array([0, 0, 0, 3]),\n",
       "  array([ 55,   3, 162,  19]),\n",
       "  array([14,  8,  4, 10]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = unk_n_grams(tagged, n)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12134,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_n_grams(tagged_sent, n=4):\n",
    "    \"\"\"\n",
    "    This function takes a (sentence, tags) tuple:\n",
    "    ('Here is an example', 'tag1 tag2 tag3 tag4')\n",
    "    \n",
    "    Returns a list arrays, one array for each unknown word:\n",
    "    The first vector contains padded n-grams combinations (X).\n",
    "    The second vector is the reversed X (X_rev).\n",
    "    The third vector is the padded n_grams for the tags (X_tag).\n",
    "    The fourth vector is the reversed X_tag (X_tag_rev).\n",
    "    \n",
    "    All returned sequences are already pre-padded with 0.\n",
    "    The maximum length of the sequences is n-1, with n being the\n",
    "    desired number of n-grams.\n",
    "    \"\"\"\n",
    "    text = list(tagged_sent)[0]\n",
    "    tags = list(tagged_sent)[1]\n",
    "    \n",
    "    unk = [i for i, word in enumerate(text) if word=='<unk>']\n",
    "    \n",
    "    unknowns = []\n",
    "    \n",
    "    for i in unk:\n",
    "        \n",
    "        x = text[i+1:i+n+1][::-1]\n",
    "        x_tag = tags[i+1:i+n+1][::-1]\n",
    "        x_rev = text[::-1][i:i+n][::-1]\n",
    "        x_tag_rev = tags[::-1][i:i+n][::-1]\n",
    "        \n",
    "\n",
    "        X = [0]*(n-len(x)) + x\n",
    "        X_tag = [0]*(n-len(x_tag)) + x_tag\n",
    "        X_rev = [0]*(n-len(x_rev)) + x_rev\n",
    "        X_tag_rev = [0]*(n-len(x_tag_rev)) + x_tag_rev\n",
    "        \n",
    "        unknowns += [np.array([X, X_tag, X_rev, X_tag_rev])]\n",
    "        \n",
    "    return unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "\n",
    "text = list(tagged)[0]\n",
    "tags = list(tagged)[1]\n",
    "\n",
    "unk = [i for i, word in enumerate(text) if word=='<unk>']\n",
    "\n",
    "unknowns = []\n",
    "x = []\n",
    "\n",
    "for i in unk:\n",
    "    x_rev = text[::-1][i:i+n][::-1]\n",
    "    unknowns.append(x_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, '<unk>', 752, 25, 46, 55, 3, 162, 19, '<unk>', 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, '<unk>', 19, 162, 3, 55, 46, 25, 752, '<unk>', 4, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[2-1::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 55,  46,  25, 752],\n",
       "        [ 14,  27,  22,   1],\n",
       "        [ 55,   3, 162,  19],\n",
       "        [ 14,   8,   4,  10]]),\n",
       " array([[0, 0, 0, 2],\n",
       "        [0, 0, 0, 3],\n",
       "        [0, 0, 1, 4],\n",
       "        [0, 0, 2, 1]])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_n_grams(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[55, 3, 162, 19], [1, 4]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
