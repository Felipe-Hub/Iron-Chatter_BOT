{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, RepeatVector, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from keras.models import load_model\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import keras\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, max_length, seed_text):\n",
    "    if seed_text == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        in_text = seed_text\n",
    "        n_words = 1\n",
    "        n_preds = 5 #number of words to predict for the seed text\n",
    "        pred_words = \"\"\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            # pre-pad sequences to a fixed length\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "            # predict probabilities for each word\n",
    "            proba = model.predict(encoded, verbose=0).flatten()\n",
    "            #take the n_preds highest probability classes \n",
    "            yhat = numpy.argsort(-proba)[:n_preds] \n",
    "            # map predicted words index to word\n",
    "            out_word = ''\n",
    "\n",
    "            for _ in range(n_preds):\n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index == yhat[_] and word not in stopwords:\n",
    "                        out_word = word\n",
    "                        pred_words += ' ' + out_word\n",
    "                        #print(out_word)\n",
    "                        break\n",
    "\n",
    "\n",
    "        return pred_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "rev_model = load_model('rev_model.h5')\n",
    "\n",
    "#load tokeniser and max_length\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    \n",
    "with open('max_length.pkl', 'rb') as f:\n",
    "    max_length = pickle.load(f)\n",
    "    \n",
    "#loading stopwords to improve relevant word predictions    \n",
    "stopwords= open('stopwords').read().split()\n",
    "\n",
    "#load spacy GloVe Model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find and set embeddings for OOV words\n",
    "def set_embedding_for_oov(doc):\n",
    "    #checking for oov words and adding embedding\n",
    "    for token in doc:\n",
    "        if token.is_oov == True:\n",
    "            before_text = doc[:token.i].text\n",
    "            after_text = str(array(doc)[:token.i:-1]).replace('[','').replace(']','')\n",
    "\n",
    "            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
    "            pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
    "            \n",
    "            embedding = numpy.zeros((300,))\n",
    "\n",
    "            i=len(before_text)\n",
    "            print('Words predicted from forward sequence model:')\n",
    "            for word in pred_before:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            i=len(after_text)\n",
    "            print('Words predicted from reverse sequence model:')\n",
    "            for word in pred_after:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            nlp.vocab.set_vector(token.text, embedding)\n",
    "            print(token.text,nlp.vocab.get_vector(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('i livwgffe in london ')\n",
    "set_embedding_for_oov(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(nlp('livwgffe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word):\n",
    "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [w.orth_ for w in by_similarity[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = nlp('i live in lndn ')\n",
    "set_embedding_for_oov(test1)\n",
    "nlp.vocab.get_vector('lndn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('lndn').similarity(nlp('London'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(nlp('lndn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = nlp('i play fidditch at school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_embedding_for_oov(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(nlp('fidditch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('fidditch').similarity(nlp('sport'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
