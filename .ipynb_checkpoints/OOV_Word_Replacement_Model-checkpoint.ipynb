{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for preparing text data into sequences for training \n",
    "def data_sequencing(data):   \n",
    "    # integer encode sequences of words\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    with open('tokenizer.pkl', 'wb') as f: # Save the tokeniser by pickling it\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    encoded = tokenizer.texts_to_sequences(data)[0]\n",
    "    # retrieve vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocabulary Size: %d' % vocab_size)\n",
    "    \n",
    "    # create line-based sequences\n",
    "    sequences = list()\n",
    "    rev_sequences = list()\n",
    "    for sentence in data:\n",
    "        encoded = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        rev_encoded = encoded[::-1]\n",
    "        for i in range(1, len(encoded)):\n",
    "            sequence = encoded[:i+1]\n",
    "            rev_sequence = rev_encoded[:i+1]\n",
    "            sequences.append(sequence)\n",
    "            rev_sequences.append(rev_sequence)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    \n",
    "    #find max sequence length \n",
    "    max_length = max([len(seq) for seq in sequences])\n",
    "    with open('max_length.pkl', 'wb') as f: # Save max_length by pickling it\n",
    "        pickle.dump(max_length, f)\n",
    "    print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "    # pad sequences and create the forward sequence\n",
    "    sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "    # split into input and output elements\n",
    "    sequences = array(sequences)\n",
    "    X, y = sequences[:,:-1],sequences[:,-1]\n",
    "    \n",
    "    #pad sequences and create the reverse sequencing\n",
    "    rev_sequences = pad_sequences(rev_sequences, maxlen=max_length, padding='pre')\n",
    "    # split into input and output elements\n",
    "    rev_sequences = array(rev_sequences)\n",
    "    rev_X, rev_y = rev_sequences[:,:-1],rev_sequences[:,-1]\n",
    "\n",
    "    return X,y,rev_X,rev_y,max_length,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'Conversation'\n",
    "files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "for filepath in files_list:\n",
    "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len( con ) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[ 1 : ]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append( ans )\n",
    "        elif len( con )> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1894\n",
      "Total Sequences: 7528\n",
      "Max Sequence Length: 72\n"
     ]
    }
   ],
   "source": [
    "#returning forward and reverse sequences along with max sequence \n",
    "#length from the data \n",
    "data = questions + answers\n",
    "data = list(set([d for d in data if type(d) is str]))\n",
    "\n",
    "X, y, rev_X, rev_y, max_length, vocab_size = data_sequencing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 71, 128)           242432    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1894)              486758    \n",
      "=================================================================\n",
      "Total params: 992,358\n",
      "Trainable params: 992,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define forward sequence model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_length-1))\n",
    "#model.add(LSTM(100))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 71, 128)           242432    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1894)              486758    \n",
      "=================================================================\n",
      "Total params: 992,358\n",
      "Trainable params: 992,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define reverse model\n",
    "rev_model = Sequential()\n",
    "rev_model.add(Embedding(vocab_size, 128, input_length=max_length-1))\n",
    "#rev_model.add(LSTM(100))\n",
    "rev_model.add(Bidirectional(LSTM(128)))\n",
    "rev_model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(rev_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6022 samples, validate on 1506 samples\n",
      "Epoch 1/2\n",
      " - 124s - loss: 7.0458 - acc: 0.0287 - val_loss: 6.4791 - val_acc: 0.0319\n",
      "Epoch 2/2\n",
      " - 80s - loss: 6.2622 - acc: 0.0344 - val_loss: 6.6515 - val_acc: 0.0359\n"
     ]
    }
   ],
   "source": [
    "# compile forward sequence network\n",
    "# loss is set to sparse_cat_cross because of multiple classes and no one-hot encoding\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(0.005), metrics=['acc'])\n",
    "# fit network\n",
    "model.fit(X, y,batch_size=512, epochs=200, verbose=2, shuffle=True, validation_split=0.2)\n",
    "# save the model to file\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6022 samples, validate on 1506 samples\n",
      "Epoch 1/2\n",
      " - 81s - loss: 6.8006 - accuracy: 0.0385 - val_loss: 6.2235 - val_accuracy: 0.0425\n",
      "Epoch 2/2\n",
      " - 52s - loss: 5.9038 - accuracy: 0.0448 - val_loss: 6.3312 - val_accuracy: 0.0425\n"
     ]
    }
   ],
   "source": [
    "# compile reverse sequence network\n",
    "rev_model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(0.005), metrics=['accuracy'])\n",
    "# fit network\n",
    "rev_model.fit(rev_X, rev_y,batch_size=512, epochs=200, verbose=2, shuffle=True, validation_split=0.2)\n",
    "# save the model to file\n",
    "rev_model.save('rev_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
